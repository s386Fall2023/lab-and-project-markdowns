# Semester Project
## Tier 2:
#### Part 1: Choose a Topic
* Select a topic that is both personally interesting and broadly appealing.
* Formulate a research question that is original, creative, compelling, and not overly trivial.
* Ensure the research question can be explored and answered using data.
* **TASKS:**
    - [ ] Brainstorm potential topics and related questions.
    - [ ] Solicit feedback from fellow students on the topic's general interest, creativity/originality, and scope of the question.
    - [ ] Choose a final topic and related question.
    - [ ] Document the feedback process and how it enhanced the general interest, scope, and originality of your final topic (include this documentation in the `feedback` folder of the repository - refer to Part 2).

#### Part 2: Data Collection and Cleaning
* In this part you will gather publicly available data to create an original dataset and summarize your process in a blog post.
* **REQUIREMENTS** 
    - [ ] Assemble data through API queries and/or web scraping using BeautifulSoup and/or Selenium, or using a Python API wrapper (utilizing multiple sources is encouraged).
    - [ ] The dataset should comprise at least:
        * 300 observations 
        * 6 informative features (an "ID" column is generally not considered informative)
        * Two numeric features
        * Two categorical features
    - [ ] Ensure that the data is comprehensive enough to support detailed Exploratory Data Analysis (EDA) and potential future predictive modeling.
* **TASKS**
    - [ ] Compile the dataset according to the specified requirements.
    - [ ] Create a public GitHub repository including:
        * Well-organized and documented Python code for scraping and cleaning (be cautious not to include private API or authentication keys).
        * The dataset (if privacy concerns exist, consult with me regarding data inclusion in the repository).
        * A Readme.md file explaining the repository's purpose and contents.
    - [ ] Craft a blog post encompassing:
        * Introduction and motivation for your chosen topic and research question.
        * A description of how data was collected and cleaned.
        * An explanation of the ethical considerations and measures taken during web scraping.
        * A concluding statement.
    - [ ] Provide a link to the data and code repository in your blog post.


#### Part 3: Exploratory Data Analysis
* In this part you will conduct an initial exploratory data analysis (EDA) of your dataset and present key findings in a blog post.
* **REQUIREMENTS**
    - [ ] The EDA should feature 6-10 figures, including various types (e.g., boxplots, scatterplots, maps).
* **TASKS**
    - [ ] Develop a list of 4-5 questions you intend to explore within your dataset.
    - [ ] Create graphs and tables to address these questions.
    - [ ] Generate new questions and insights as you proceed with the EDA.
    - [ ] Select the 6-10 most compelling graphs and tables to discuss.
    - [ ] Add well-documented Python code for your EDA to the GitHub repository.
    - [ ] Craft a blog post encompassing:
        * Introduction and motivation for your chosen topic and research question (can be a brief review citing that more details are in your previous post).
        * The visualizations you chose previously along with commentary and insights.
        * A summary of the most intriguing findings from your data and how the EDA 
        * A concluding statement.
    - [ ] Include a link to the data and code repository in your blog post.
 
#### Part 4: Present Your Data
* In this part, you will create a dashboard using Streamlit (or another *Python*-based dashboard package) to  present your findings and insights from the previous stages of your project. This dashboard serves as a user-friendly interface for visualizing and interacting with your data.

* **TASKS**
    - [ ] Use the data visualizations and insights generated during your Exploratory Data Analysis (Part 3) to populate the dashboard (or just use these visualizations as a starting point).
    - [ ] Ensure that your dashboard provides an intuitive and informative user experience, making it easy for viewers to explore the dataset and understand the key findings.
    - [ ] Incorporate interactive elements, such as dropdown menus, sliders, or filters, to allow users to customize their data exploration experience.
    - [ ] Include descriptive captions or tooltips for each visualization to provide context and explanations for your findings.
    - [ ] Test the functionality of your dashboard thoroughly to identify and resolve any issues or bugs.
    - [ ] Consider the visual design and layout of your dashboard to make it visually appealing and user-friendly.
    - [ ] Integrate navigation options or links to your blog post and the GitHub repository for those who want to delve deeper into your project.
    - [ ] Ensure that your dashboard effectively conveys the story and insights from your data, catering to both technical and non-technical audiences.
 
